{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c1763e-6a8b-4ce5-97a4-445f27e5467a",
   "metadata": {},
   "source": [
    "# train_style_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd38f3",
   "metadata": {},
   "source": [
    "Model Structure:\n",
    "- Encoder: Full Pretrained VGG19 (all convolutional layers, frozen)\n",
    "- AdaIN: Adaptive Instance Normalization aligning content/style features\n",
    "- Decoder: Lightweight CNN with Upsample + Conv layers to reconstruct image\n",
    "\n",
    "Usage examples (run on 4Ã—A100 servers via torchrun):\n",
    "\n",
    "### Single GPU\n",
    "'''\n",
    "python train_style_transfer.py \\\n",
    "    --data_root /home/cc/img-dataset \\\n",
    "    --global_batch_size 32 \\\n",
    "    --micro_batch_size 8 \\\n",
    "    --epochs 5 \\\n",
    "    --precision fp32 \\\n",
    "    --strategy none \\\n",
    "    --export_path ./stylizer.pt\n",
    "'''\n",
    "\n",
    "### 4-GPU DDP\n",
    "'''\n",
    "torchrun --nproc_per_node=4 train_style_transfer.py \\\n",
    "    --data_root /home/cc/img-dataset \\\n",
    "    --global_batch_size 128 \\\n",
    "    --micro_batch_size 8 \\\n",
    "    --epochs 5 \\\n",
    "    --precision amp \\\n",
    "    --strategy ddp \\\n",
    "    --export_path ./stylizer_ddp.pt\n",
    "'''\n",
    "\n",
    "### 4-GPU FSDP\n",
    "'''\n",
    "torchrun --nproc_per_node=4 train_style_transfer.py \\\n",
    "    --data_root /home/cc/img-dataset \\\n",
    "    --global_batch_size 128 \\\n",
    "    --micro_batch_size 8 \\\n",
    "    --epochs 5 \\\n",
    "    --precision amp \\\n",
    "    --strategy fsdp \\\n",
    "    --export_path ./stylizer_fsdp.pt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9ee2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "    from torch.distributed.fsdp import CPUOffload, ShardingStrategy\n",
    "except ImportError:\n",
    "    FSDP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4528d1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Dataset ----------\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_root, transform):\n",
    "        self.content_paths = glob.glob(os.path.join(content_dir, '*'))\n",
    "        self.style_paths = []  # list of (path, label)\n",
    "        style_dirs = sorted(os.listdir(style_root))\n",
    "        for label in style_dirs:\n",
    "            cls_dir = os.path.join(style_root, label)\n",
    "            if os.path.isdir(cls_dir):\n",
    "                for p in glob.glob(os.path.join(cls_dir, '*')):\n",
    "                    self.style_paths.append((p, int(label)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.content_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_img = Image.open(self.content_paths[idx]).convert('RGB')\n",
    "        s_path, label = random.choice(self.style_paths)\n",
    "        s_img = Image.open(s_path).convert('RGB')\n",
    "        return self.transform(c_img), self.transform(s_img), label\n",
    "\n",
    "# ---------- AdaIN ----------\n",
    "def adain(content_feat, style_feat, eps=1e-5):\n",
    "    c_mean = content_feat.mean(dim=[2,3], keepdim=True)\n",
    "    c_std = content_feat.std(dim=[2,3], keepdim=True)\n",
    "    s_mean = style_feat.mean(dim=[2,3], keepdim=True)\n",
    "    s_std = style_feat.std(dim=[2,3], keepdim=True)\n",
    "    norm = (content_feat - c_mean) / (c_std + eps)\n",
    "    return norm * s_std + s_mean\n",
    "\n",
    "# ---------- Model ----------\n",
    "class StyleTransferModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.enc = nn.Sequential(*list(vgg.children()))\n",
    "        for p in self.enc.parameters(): p.requires_grad = False\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Conv2d(512,256,3,padding=1), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(256,128,3,padding=1), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(128,64,3,padding=1), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64,3,3,padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, content, style):\n",
    "        c_feat = self.enc(content)\n",
    "        s_feat = self.enc(style)\n",
    "        t = adain(c_feat, s_feat)\n",
    "        out = self.dec(t)\n",
    "        return out, t, s_feat\n",
    "\n",
    "# ---------- Inference Wrapper ----------\n",
    "class Stylizer(nn.Module):\n",
    "    def __init__(self, model, style_dir, transform, device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.style_feats = []\n",
    "        style_dirs = sorted(os.listdir(style_dir))\n",
    "        for label in style_dirs:\n",
    "            cls_dir = os.path.join(style_dir, label)\n",
    "            p = glob.glob(os.path.join(cls_dir, '*'))[0]\n",
    "            img = transform(Image.open(p).convert('RGB')).unsqueeze(0).to(device)\n",
    "            with torch.no_grad(): feat = self.model.enc(img)\n",
    "            self.style_feats.append(feat)\n",
    "\n",
    "    def forward(self, content, style_label):\n",
    "        if isinstance(style_label, int):\n",
    "            feat = self.style_feats[style_label].repeat(content.size(0),1,1,1)\n",
    "        else:\n",
    "            feat = torch.stack([self.style_feats[i] for i in style_label],0)\n",
    "        t = adain(self.model.enc(content), feat)\n",
    "        return self.model.dec(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2902c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Training & Export ----------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_root', required=True)\n",
    "    parser.add_argument('--global_batch_size', type=int, default=32)\n",
    "    parser.add_argument('--micro_batch_size', type=int, default=8)\n",
    "    parser.add_argument('--epochs', type=int, default=5)\n",
    "    parser.add_argument('--precision', choices=['fp32','amp'], default='fp32')\n",
    "    parser.add_argument('--strategy', choices=['none','ddp','fsdp'], default='none')\n",
    "    parser.add_argument('--style_w', type=float, default=10.0)\n",
    "    parser.add_argument('--tv_w', type=float, default=1e-6)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--export_path', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    distributed = args.strategy in ['ddp','fsdp']\n",
    "    if distributed:\n",
    "        torch.distributed.init_process_group('nccl')\n",
    "        rank = torch.distributed.get_rank(); world = torch.distributed.get_world_size()\n",
    "    else: rank, world = 0,1\n",
    "    assert args.global_batch_size % (args.micro_batch_size*world)==0\n",
    "    accum = args.global_batch_size//(args.micro_batch_size*world)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    ds = StyleTransferDataset(\n",
    "        os.path.join(args.data_root,'content'),\n",
    "        os.path.join(args.data_root,'style'),\n",
    "        transform\n",
    "    )\n",
    "    sampler = DistributedSampler(ds) if distributed else None\n",
    "    loader = DataLoader(ds, batch_size=args.micro_batch_size, sampler=sampler,\n",
    "                        shuffle=not distributed, num_workers=4, pin_memory=True)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    model = StyleTransferModel().to(device)\n",
    "    opt = optim.Adam(model.dec.parameters(), lr=args.lr)\n",
    "    scaler = torch.cuda.amp.GradScaler() if args.precision=='amp' else None\n",
    "\n",
    "    if args.strategy=='ddp': model=nn.parallel.DistributedDataParallel(model, device_ids=[torch.cuda.current_device()])\n",
    "    if args.strategy=='fsdp': model=FSDP(model, cpu_offload=CPUOffload(False), sharding_strategy=ShardingStrategy.FULL_SHARD)\n",
    "\n",
    "    log=[]\n",
    "    for e in range(args.epochs):\n",
    "        if sampler: sampler.set_epoch(e)\n",
    "        start=time.time(); opt.zero_grad()\n",
    "        for i,(c,s,_) in enumerate(loader):\n",
    "            c,s=c.to(device),s.to(device)\n",
    "            with torch.cuda.amp.autocast(args.precision=='amp'):\n",
    "                out,t,sf=model(c,s)\n",
    "                of=model.enc(out)\n",
    "                l_c=F.mse_loss(of,t)\n",
    "                l_s=F.mse_loss(of.mean([2,3]), sf.mean([2,3]))+F.mse_loss(of.std([2,3]), sf.std([2,3]))\n",
    "                l_tv = torch.sum(torch.abs(out[:,:,1:]-out[:,:,:-1]))+torch.sum(torch.abs(out[:,:,:,1:]-out[:,:,:,:-1]))\n",
    "                loss=(l_c+args.style_w*l_s+args.tv_w*l_tv)/accum\n",
    "            if scaler: scaler.scale(loss).backward()\n",
    "            else: loss.backward()\n",
    "            if (i+1)%accum==0:\n",
    "                if scaler: scaler.step(opt); scaler.update()\n",
    "                else: opt.step()\n",
    "                opt.zero_grad()\n",
    "        t_e=time.time()-start; m=torch.cuda.max_memory_allocated()/1e9\n",
    "        if rank==0: log.append((args.strategy,world,e,t_e,m)); print(f\"Epoch{e} {t_e:.1f}s mem{m:.2f}GB\")\n",
    "\n",
    "    if rank==0:\n",
    "        base = model.module if hasattr(model,'module') else model\n",
    "        stylizer = Stylizer(base, os.path.join(args.data_root,'style'), transform, device)\n",
    "        scripted = torch.jit.script(stylizer)\n",
    "        scripted.save(args.export_path)\n",
    "        print(f\"Exported stylizer to {args.export_path}\")\n",
    "\n",
    "if __name__=='__main__': main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
